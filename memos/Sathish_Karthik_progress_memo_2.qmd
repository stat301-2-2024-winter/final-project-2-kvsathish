---
title: "Progress Memo 2"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Karthik Sathish"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---



::: {.callout-tip icon=false}

## Github Repo Link

[My Github Repo Link](https://github.com/stat301-2-2024-winter/final-project-2-kvsathish.git)

:::



```{r}
#| echo: false

# packages and datasets

library(tidyverse)
library(skimr)
library(ggplot2)
library(here)
library(naniar)
library(knitr)
library(rsample)
library(tidymodels)
library(doParallel)
set.seed(4231)

load(here("data/bball_players.rda"))

load(here("results/bball_split.rda"))
load(here("results/bball_train.rda"))
load(here("results/bball_test.rda"))
load(here("results/bball_folds.rda"))
load(here("results/basic_rec.rda"))

load(here("results/null_fit.rda"))
load(here("results/logreg_fit.rda"))
```





## New Analysis of Target Variable

```{r}
#| label: fig-1
#| fig-cap: Distribution of the target variable in the `bball_players` dataset
#| echo: false

bball_train |> 
  ggplot(aes(x = pick)) +
  geom_bar() +
  labs(title = "Distribution of `pick`",
       y = NULL) +
  theme_minimal()
```

@fig-1 shows a pretty lobsided bar chart for the variable `pick`. This is because there are much more players that went undrafted ("No") than players that were picked in the draft ("Yes"). I plan to address this issue by manipulating the data in a different way. More about this problem and how I'll address it can be found in the **Progress** section.



### Data Recap

The source of my data is a user on `Kaggle.com`, named Aditya Kumar. They are a computer science major at the University of Southern California with roots in Los Angeles, California. The data card is named `College Basketball 2009-2021 + NBA Advanced Stats`^[Found on [https://www.kaggle.com/datasets/adityak2003/college-basketball-players-20092021/data](https://www.kaggle.com/datasets/adityak2003/college-basketball-players-20092021/data)]. Although all the stats were compiled by Aditya, the raw statistical data should be credited to Bart Torvik^[Found on [https://barttorvik.com/](https://barttorvik.com/)].

As for the current analysis, I am utilizing a singular dataset called `CollegeBasketballPlayers2009-2021.csv` to compare over 4500 college basketball players from 2009 to 2021. However, I may also utilize the `CollegeBasketballPlayers2022.csv` if I feel as though more drafted individuals are needed for analysis. If I need to join these datasets together, I plan on seamlessly aligning variables based on similarity and function.


## Assessment Metric

I plan to use the **accuracy** metric to assess the performance of the two completed models below.


## Plan for Analysis


#### Initial Splitting and Folds

Since the `bball_players` dataset has approximately 60,000 observations and only about 1,500 "yes" values for the `pick` variable (indicating a player was drafted), I first decided to down sample the data with a proportion of 0.05. Following this, I made an initial split with a proportion of 0.8 to gather appropriate training and testing data for analysis. Also, I stratified the data by the `pick` variable to try and balance the players that were drafted vs undrafted. It also makes sure all the predictor variables are represented fairly.


```{r}
#| label: tbl-1
#| tbl-cap: Dimensions of the training and testing data within `bball_split`
#| echo: false

# dimensions of the datasets 
train_dims <- dim(bball_train)
test_dims <- dim(bball_test)

dims_tibble <- tibble(
  Dataset = c("Training", "Testing"),
  Rows = c(train_dims[1], test_dims[1]),
  Columns = c(train_dims[2], test_dims[2])
)

dims_tibble |> 
  kable(align = "c", caption = "Dimensions of the datasets in `bball_split`")

```

@tbl-1 above reveals the corresponding dimensions for the training and testing data after all the necessary splitting has occurred.

After the data was split, I made sure to conduct resampling by utilizing cross-validation. In total, there were 50 folds (10 folds repeated 5 times) that were created using the `vfold_cv()` function and stratifying by the `pick` variable.

#### Models and Recipes

The model types that I am planning to utilize include a null (baseline), logistic regression, tuned random forest, tuned boosted trees, tuned k-nearest neighbors, and naive bayes models. In total, I plan to create six models.

In terms of recipes for the six models above, I plan to make at least four recipes. Two distinct recipes will be made; one that is linear-based (for the null and logistic regression models) and one that is trees-based (for the random forest, boosted trees, and k-nearest neighbors models). As for the two distinct recipes above, variations of the recipes will be created to account for the interactions between certain variables. Also, another distinct recipe will be made for the naive bayes model that excludes the `step_dummy` function.


## Completed Models

Currently, I have completed two separate models and fitted them to the resamples. One of the models is a null (baseline) model for my classification problem. The other model is a logistic regression model. For both models, a very basic kitchen sink recipe was utilized for the fitting.

## Metric Evaluation of Models

```{r}
#| label: tbl-2
#| tbl-cap: Accuracy metric for the Null and Logistic Regression Model
#| echo: false

# collect accuracy metric
null_accuracy <- null_fit |> 
  collect_metrics() |> 
  filter(.metric == "accuracy")

logreg_accuracy <- logreg_fit |> 
  collect_metrics() |> 
  filter(.metric == "accuracy")

# format into a table
accuracy_table <- bind_rows(
  mutate(null_accuracy, model = "Null Model"),
  mutate(logreg_accuracy, model = "Logistic Regression")
) |> 
  select(model, everything())

kable(accuracy_table, caption = "Accuracy Metrics")
```

@tbl-2 above successfully displays my chosen assessment metric (accuracy) and the corresponding accuracy values for two types of models. The models analyzed include the finished model fits: Null Model and Logistic Regression. Across all folds, the null model reflected an accuracy of its predictions as approximately 98.0% on average, which is extraordinarily high. This could be due to a very non-uniform distribution of `pick`, resulting in the model defaulting to "no" as a prediction. My plan to account for this can be found in the **Progress* section. For the Logistic Regression model across all folds, the accuracy of its predictions is found to be approximately 87.5% on average, which is also high. Both means found in the table are justified by the extremely low standard errors (less than 0.0051 for both).


## Progress

#### Summary

In terms of this second memo, I feel as though I have made significant progress for the final project. Prior to completing the two fitted models above, I created a simple recipe that is based on the kitchen sink approach. First, I used the `step_rm` function to remove all the variables that are not related to the target variable in any way. Then, I used the `step_dummy` function to create dummy variables that account for all the categorical variables acting as predictors. After that, I used `step_zv` to remove variables that had no variability across values and used `step_impute_mean` to impute any missing values with the mean of all other values for each of the numerical variables acting as predictors. Finally, I standardized all the predictor variables with the `step_normalize` function. This basic recipe served as an important tool to examine the fitted baseline model as well as the fitted logistic model. On top of this, I have also planned out what other models I plan to use and what recipes will correspond well with each of the models. Next steps include appropriately tuning each of the new models and reflecting as assessment metric for all the models to evaluate which model is the best at predicting which college basketball players will be drafted to the NBA.


#### Issues (Addressing Big Data Concern)

The biggest concern with my work done so far is that the distribution of the `pick` variable for my data analysis is not uniform. In fact, it is very lobsided where there are many more college basketball players that are undrafted compared to drafted. This leads to the major problem seen with computing the assessment metric above for the completed models. To address this issue, I plan to create a new dataset. First, I will create a `pick_dataset` that includes only the players that got drafted ("yes" value in the `pick` column). Then, I will create a `non_pick_dataset` that includes only undrafted players, and I'll make sure to stratify the new dataset by `year` to fairly collect players from 2009-2021. With the `non_pick_dataset`, I will then down sample it to approximately match the number of observations in the `pick_dataset`. Finally, I will utilize the `bind_rows()` function to combine both datasets into one complete dataset for analysis. Once this new suitable dataset for predictive modelling is created, I will repeat the same fitting or tuning process for each of the chosen models. The only thing to keep in mind is that the Null Model will be useless. It will reflect a value near 0.5 because there are approximately the same number of undrafted and drafted players. The balanced data renders the null model as useless because null models only make predictions off the majority class. This will be kept in mind as I choose a new baseline model prior to thorough analysis. Outside of this major issue, I am not concerned with much else. If more observations are needed, I will also incorporate players from the 2022 dataset. All in all, I feel confident moving forward with the final project.

