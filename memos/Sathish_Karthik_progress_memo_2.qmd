---
title: "Progress Memo 2"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Karthik Sathish"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---



::: {.callout-tip icon=false}

## Github Repo Link

[My Github Repo Link](https://github.com/stat301-2-2024-winter/final-project-2-kvsathish.git)

:::



```{r}
#| echo: false

# packages and datasets

library(tidyverse)
library(skimr)
library(ggplot2)
library(here)
library(naniar)
library(knitr)
library(rsample)
library(tidymodels)
library(doParallel)
set.seed(4231)

load(here("data/bball_players.rda"))

load(here("results/bball_split.rda"))
load(here("results/bball_train.rda"))
load(here("results/bball_test.rda"))
load(here("results/bball_folds.rda"))
load(here("results/basic_rec.rda"))

load(here("results/null_fit.rda"))
load(here("results/logreg_fit.rda"))
```





## New Analysis of Target Variable

```{r}
#| label: fig-1
#| fig-cap: Distribution of the target variable in the `bball_players` dataset
#| echo: false

bball_train |> 
  ggplot(aes(x = pick)) +
  geom_bar() +
  labs(title = "Distribution of `pick`",
       y = NULL) +
  theme_minimal()
```

@fig-1 shows a pretty lobsided bar chart for the variable `pick`. This is because there are much more players that went undrafted ("No") than players that were picked in the draft ("Yes"). I plan to address this issue by manipulating the data in a different way. More about this problem and how I'll address it can be found in the **Progress** section.



### Data Recap

The source of my data is a user on `Kaggle.com`, named Aditya Kumar. They are a computer science major at the University of Southern California with roots in Los Angeles, California. The data card is named `College Basketball 2009-2021 + NBA Advanced Stats`^[Found on [https://www.kaggle.com/datasets/adityak2003/college-basketball-players-20092021/data](https://www.kaggle.com/datasets/adityak2003/college-basketball-players-20092021/data)]. Although all the stats were compiled by Aditya, the raw statistical data should be credited to Bart Torvik^[Found on [https://barttorvik.com/](https://barttorvik.com/)].

As for the current analysis, I am utilizing a singular dataset called `CollegeBasketballPlayers2009-2021.csv` to compare over 4500 college basketball players from 2009 to 2021. However, I may also utilize the `CollegeBasketballPlayers2022.csv` if I feel as though more drafted individuals are needed for analysis. If I need to join these datasets together, I plan on seamlessly aligning variables based on similarity and function.


## Assessment Metric

I plan to use the **accuracy** metric to assess the performance of the two models below.


## Plan for Analysis


#### Initial Splitting and Folds

Since the `bball_players` dataset has approximately 60,000 observations and only about 1,500 "yes" values for the `pick` variable (indicating a player was drafted), I first decided to down sample the data with a proportion of 0.05. Following this, I made an initial split with a proportion of 0.8 to gather appropriate training and testing data for analysis. Also, I stratified the data by the `pick` variable to try and balance the players that were drafted vs undrafted. It also makes sure all the predictor variables are represented fairly.


```{r}
#| label: tbl-1
#| tbl-cap: Dimensions of the training and testing data within `bball_split`
#| echo: false

# dimensions of the datasets 
train_dims <- dim(bball_train)
test_dims <- dim(bball_test)

dims_tibble <- tibble(
  Dataset = c("Training", "Testing"),
  Rows = c(train_dims[1], test_dims[1]),
  Columns = c(train_dims[2], test_dims[2])
)

dims_tibble |> 
  kable(align = "c", caption = "Dimensions of the datasets in `bball_split`")

```

@tbl-1 above reveals the corresponding dimensions for the training and testing data after all the necessary splitting has occurred.

After the data was split, I made sure to conduct resampling by utilizing cross-validation. In total, there were 50 folds (10 folds repeated 5 times) that were created using the `vfold_cv()` function and stratifying by the `pick` variable.

#### Models and Recipes

The model types that I am planning to utilize include a null (baseline), logistic regression, tuned random forest, tuned boosted trees, tuned k-nearest neighbors, and naive bayes models. In total, I plan to create six models.

In terms of recipes for the six models above, I plan to make at least four recipes. Two distinct recipes will be made; one that is linear-based (for the null and logistic regression models) and one that is trees-based (for the random forest, boosted trees, and k-nearest neighbors models). As for the two distinct recipes above, variations of the recipes will be created to account for the interactions between certain variables. Also, another distinct recipe will be made for the naive bayes model that excludes the `step_dummy` function.


## Completed Models

have two model types defined and fitted to the resamples: null/baseline model and one other model type (likely standard linear or logistic model):
The recipe can be very basic since we just want to see if you can get things running

## Metric Evaluation of Models

```{r}
#| label: tbl-2
#| tbl-cap: Accuracy metric for the Null and Logistic Regression Model
#| echo: false

# collect accuracy metric
null_accuracy <- null_fit |> 
  collect_metrics() |> 
  filter(.metric == "accuracy")

logreg_accuracy <- logreg_fit |> 
  collect_metrics() |> 
  filter(.metric == "accuracy")

# format into a table
accuracy_table <- bind_rows(
  mutate(null_accuracy, model = "Null Model"),
  mutate(logreg_accuracy, model = "Logistic Regression")
) |> 
  select(model, everything())

kable(accuracy_table, caption = "Accuracy Metrics")
```

demonstrate their fits are successful by displaying a table that contains each model's assessment metric value.

## Progress

summarize their progress, where they are at, and what their next steps will be. Identifying and potential issues or concerns would be a good idea too.

