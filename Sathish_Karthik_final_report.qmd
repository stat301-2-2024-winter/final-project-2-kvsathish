---
title: "The NBA Draft: Predicting Draft Selection among College Basketball Players"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Karthik Sathish"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---



::: {.callout-tip icon=false}

## Github Repo Link

[My Github Repo Link](https://github.com/stat301-2-2024-winter/final-project-2-kvsathish.git)

:::

```{r}
#| echo: false

# load packages and datasets
library(tidyverse)
library(skimr)
library(here)
library(naniar)
library(knitr)
library(tidymodels)
set.seed(423)

cbb_players <- read_csv("data/CollegeBasketballPlayers2009-2021.csv")
load(here("data/bball_players.rda"))

# handle common conflicts
tidymodels_prefer()



```


## Introduction

The NBA Draft is a special event where all 30 teams within the National Basketball Association have the opportunity to select young basketball player(s) for the future of their respective franchise. While there has been a recent transition to drafting players from overseas, I wanted to look into what specifically qualifies NCAA college basketball players to get selected by an NBA team in the draft. Becoming a professional basketball player is very difficult, with only about 1.2% of collegiate players making the jump to the NBA. Numerous factors play into an NBA selection, from athletic skills to prospective talent and discipline. 

However, as an avid basketball fan, I am extremely interested to see if it is possible to **predict whether a NCAA college basketball player is drafted into the NBA**, based on a range of statistics collected across all seasons played at the collegiate level. The [College Basketball Players Dataset](https://www.kaggle.com/datasets/adityak2003/college-basketball-players-20092021/data) is utilized for my thorough analysis.


The resulting prediction model might not change the world, but it piques my interest. Basketball has always been my favorite sport to watch, and it is so exciting that I am able to conduct a thorough analysis combining basketball at the collegiate and pro level. I've always thought about why some of the greatest college basketball players do not pan out in the NBA, and this statistical analysis may reveal which factors are most important for an appropriate transition to the pro level. A prediction model would be useful to identify which prospective players may translate best into the NBA. This model could also help future players mold their game toward a more specific prototype to achieve more success in the sport. Moreover, I think it would be very interesting to also possibly see how the prototype of NBA draftees had changed over time.




## Data Overview

Within the raw dataset `cbb_players`, there are 66 total variables and 61,061 total observations. Furthermore, there are 7 categorical variables as well as 59 numerical variables. In terms of missingness, there are missing values present in 35 of the variables. @tbl-missingness reveals the `missing_percentage` or the percentage of values that are missing in each column with missingness. These `NA` values are accounted for during my analysis of the dataset to make predictive models.

```{r}
#| echo: false
#| label: tbl-missingness
#| tbl-cap: Summary of Variables with Missing Values
# display missingness
missing_summary <- cbb_players |> 
  summarise_all(~mean(is.na(.)) * 100) |> 
  gather(variable, missing_percentage) |> 
  filter(missing_percentage > 0) |> 
  arrange(desc(missing_percentage))

kable(missing_summary, 
      col.names = c("Variable", "Missing Percentage"),
      caption = "Variables with Missing Values")
```


For this project, college basketball players and their respective statistics, within seasons from 2009 to 2021, will be analyzed. Specifically, the `pick` variable was chosen as the target variable within the `cbb_players` dataset. The values for this variable indicated what number pick in the NBA draft each of the players achieved. Although this variable was presented as numerical variable, `pick` was manipulated to become a categorical variable. The new values are represented as (`Yes`/`No`) for whether a player was picked or not. 


Due to the significant imbalance between college basketball players that were drafted compared to undrafted, the analysis is conducted with a much smaller version of the original dataset. The new dataset `draft_data` was utilized to create a new training and testing set that would help facilitate predictive modelling for the challenge at hand. This new dataset also accounts for the missingness above by transforming certain variables like `rec_rank` (Recruiting rank from high school) from a numerical variable to a categorical one.


![](images/draft_pic.png)

Following the creation of an approxmiately uniform dataset for the variable `pick` that also accounted for missingness, predictive modelling was viable. Plus, each of the observations (unique players) correspond to over 60 different characteristics or statistics for appropriate and thorough analysis. More information about each of the recorded stats can be found in the `bball_players_codebook` text file within the `data` subdirectory.


## Methods
Should cover the data splitting procedure and clearly identify what type of prediction problem it is. State and describe the model types you will be fitting. Describe any parameters that will be tuned. Describe what recipes will be used. Describe the resampling technique used. In some cases an extended discussion about recipe variations might be useful. Especially if students are using recipe variation to try and explore the predictive importance of certain variables. Explain the metric that will be used to compare and ultimately used to select a final model.

### Overview

Since a variety of predictor variables are utilized to predict whether a NCAA college basketball player is drafted or not, this prediction problem is identified as a simple classification problem.

### Data Splitting and Resampling

Before splitting the data, it was necessary to manipulate the dataset due to the large imbalance between college players that are drafted and those that are selected. First, created a `pick_dataset` that includes only the players that got drafted ("yes" value in the `pick` column). Then, I created a `non_pick_dataset` that includes only undrafted players, making sure to stratify the new dataset by `year` to fairly collect players from 2009-2021. With the `non_pick_dataset`, I then down sampled it to approximately match the number of observations in the `pick_dataset`. Finally, I utilized the `bind_rows()` function to combine both datasets into one complete dataset for analysis. 


Once this new suitable dataset for predictive modelling was created, the data was split so that 80% of the data could be utilized for training the models and 20% of the data could be utilized for testing the models. This split was conducted with stratified sampling by the variable `pick`, meaning there were two strata. 


Resampling was necessary to repeatedly estimate the population parameter for thorough analysis. This ensured random samples from the training data were chosen with replacement. Resampling was conducted by utilizing v-fold cross validation with 10 folds and 5 repeats. Also, this resampling process invovled stratifying the `pick` variable. In total, each model was fitted/trained 50 times. 


### Modelling and Tuning

In terms of model types, 6 different models were utilized below: 

1) Naive bayes model (`klaR`): This is my baseline model.

2) Logistic regression model (`glm`): This model is used because the dependent variable is binary, and it models the probability of an observation identifying with a certain class.


The following model types used some tuned hyperparameters with a regular grid to improve predictive capabilities:


3) Elastic net model (`glmnet`): This model uses both Ridge and Lasso to conduct a logistic regression. It selects certain features and accounts for noisy data to make predictions. 
    - penalty was tuned to prevent overfitting
    - mixture was tuned to act more like a Ridge or Lasso model for better accuracy


4) Boosted tree (`xgboost`): This model utilizes decision trees that account for results of previous trees to make new predictions.
    - min_n was tuned to control how many data points were split
    - mtry was tuned to control how many predictors were used
    - learn_rate was tuned to control the impact of new trees


5) K-nearest neighbors model (`kknn`): This model narrows the prediction features and uses Euclidean distance calculations. After that, it identifies the `K` closest points to predict values.
    - neighbors was tuned to control how many `K` points are utilized for predictions


6) Random forest model (`ranger`): This model utilizes decision trees based on random features. All the decision tree results are combined for predictions.
    - min_n was tuned to control how many data points were split
    - mtry was tuned to control how many predictors were used



### Recipes

A total of 5 recipes were created:


For the naive bayes model:

  - Basic Recipe (`nb_rec`): This preprocessing involved a standard kitchen sink recipe that utilized all the variables except those with zero variance, missingness issues, or were unrelated to the target variable. No dummy variables were used.


For the logistic regression and elastic net models:

  - Basic Recipe (`basic_rec`): This preprocessing involved a standard kitchen sink recipe that utilized all the variables except those with zero variance, missingness issues, or were unrelated to the target variable. Dummy variables were made out of the nominal variables and the data was appropriately scaled.
  
  - Variant Recipe (`off_rec`): This feature engineered recipe was created to evaluate whether a better predictive model could be produced by using solely utilizing offensive output for each basketball player. The thought process behind this recipe was that the NBA is often associated with pure offensive skill. Defense if often overlooked and it is generally thought that offensive talent and output in college translates best to the NBA. In order to test this, the preprocessing involved removing all variables unrelated to offensive output, converting nominal variables to dummy variables, imputing means for missing values for numerical variables, correlating numerical variables that are related, creating an interaction between assist percentage and turnover percentage, appropriately scaling the data, and accounting for variables with zero variance.


For the boosted tree, k-nearest neighbor, and random forest models:

  - Basic Recipe (`trees_rec`): This preprocessing involved a standard kitchen sink recipe for trees that utilized all the variables except those with zero variance, missingness issues, or were unrelated to the target variable. Dummy variables were made out of the nominal variables and the data was appropriately scaled.

  - Variant Recipe (`off_trees`): This feature engineered trees recipe was created to evaluate whether a better predictive model could be produced by using solely utilizing offensive output for each basketball player. The thought process behind this recipe was that the NBA is often associated with pure offensive skill. Defense if often overlooked and it is generally thought that offensive talent and output in college translates best to the NBA. In order to test this, the preprocessing involved removing all variables unrelated to offensive output, converting nominal variables to dummy variables, imputing means for missing values for numerical variables, correlating numerical variables that are related, appropriately scaling the data, and accounting for variables with zero variance.


### Evaluation Metric

Due to the classification problem at hand, the models will be compared with the accuracy metric. This metric measures the proportion of correct predictions out of total predictions. In order to effectively visualize the best model, a confusion matrix will be constructed.


## Model Building & Selection


## Final Model Analysis


## Conclusion


## References

For my analysis above, I am utilizing a singular dataset called `CollegeBasketballPlayers2009-2021.csv` to compare over 4500 college basketball players from 2009 to 2021.

The source of my data is a user on `Kaggle.com`, named Aditya Kumar. They are a computer science major at the University of Southern California with roots in Los Angeles, California. The data card is named `College Basketball 2009-2021 + NBA Advanced Stats`^[Found on [https://www.kaggle.com/datasets/adityak2003/college-basketball-players-20092021/data](https://www.kaggle.com/datasets/adityak2003/college-basketball-players-20092021/data)]. Although all the stats were compiled by Aditya, the raw statistical data should be credited to Bart Torvik^[Found on [https://barttorvik.com/](https://barttorvik.com/)].


### Citations


## Appendix: if needed