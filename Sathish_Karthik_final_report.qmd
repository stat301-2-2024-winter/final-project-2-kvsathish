---
title: "The NBA Draft: Predicting Draft Selection among College Basketball Players"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Karthik Sathish"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---



::: {.callout-tip icon=false}

## Github Repo Link

[My Github Repo Link](https://github.com/stat301-2-2024-winter/final-project-2-kvsathish.git)

:::

```{r}
#| echo: false

# load packages and datasets
library(tidyverse)
library(skimr)
library(here)
library(naniar)
library(knitr)
library(tidymodels)
set.seed(423)

cbb_players <- read_csv("data/CollegeBasketballPlayers2009-2021.csv")
load(here("data/bball_players.rda"))
load(here("results/bball_train.rda"))
load(here("results/bball_test.rda"))

# handle common conflicts
tidymodels_prefer()



```


## Introduction

The NBA Draft is a special event where all 30 teams within the National Basketball Association have the opportunity to select young basketball player(s) for the future of their respective franchise. While there has been a recent transition to drafting players from overseas, I wanted to look into what specifically qualifies NCAA college basketball players to get selected by an NBA team in the draft. Becoming a professional basketball player is very difficult, with only about 1.2% of collegiate players making the jump to the NBA. Numerous factors play into an NBA selection, from athletic skills to prospective talent and discipline. 

However, as an avid basketball fan, I am extremely interested to see if it is possible to **predict whether a NCAA college basketball player is drafted into the NBA**, based on a range of statistics collected across all seasons played at the collegiate level. The [College Basketball Players Dataset](https://www.kaggle.com/datasets/adityak2003/college-basketball-players-20092021/data) is utilized for my thorough analysis.


The resulting prediction model might not change the world, but it piques my interest. Basketball has always been my favorite sport to watch, and it is so exciting that I am able to conduct a thorough analysis combining basketball at the collegiate and pro level. I've always thought about why some of the greatest college basketball players do not pan out in the NBA, and this statistical analysis may reveal which factors are most important for an appropriate transition to the pro level. A prediction model would be useful to identify which prospective players may translate best into the NBA. This model could also help future players mold their game toward a more specific prototype to achieve more success in the sport. Moreover, I think it would be very interesting to also possibly see how the prototype of NBA draftees had changed over time.




## Data Overview

Within the raw dataset `cbb_players`, there are 66 total variables and 61,061 total observations. Furthermore, there are 7 categorical variables as well as 59 numerical variables. In terms of missingness, there are missing values present in 35 of the variables. @tbl-missingness reveals the `missing_percentage` or the percentage of values that are missing in each column with missingness. These `NA` values are accounted for during my analysis of the dataset to make predictive models.

```{r}
#| echo: false
#| label: tbl-missingness
#| tbl-cap: Summary of Variables with Missing Values
# display missingness
missing_summary <- cbb_players |> 
  summarise_all(~mean(is.na(.)) * 100) |> 
  gather(variable, missing_percentage) |> 
  filter(missing_percentage > 0) |> 
  arrange(desc(missing_percentage))

kable(missing_summary, 
      col.names = c("Variable", "Missing Percentage"),
      caption = "Variables with Missing Values")
```


For this project, college basketball players and their respective statistics, within seasons from 2009 to 2021, will be analyzed. Specifically, the `pick` variable was chosen as the target variable within the `cbb_players` dataset. The values for this variable indicated what number pick in the NBA draft each of the players achieved. Although this variable was presented as numerical variable, `pick` was manipulated to become a categorical variable. The new values are represented as (`Yes`/`No`) for whether a player was picked or not. 


Due to the significant imbalance between college basketball players that were drafted compared to undrafted, the analysis is conducted with a much smaller version of the original dataset. The new dataset `draft_data` was utilized to create a new training and testing set that would help facilitate predictive modelling for the challenge at hand. This new dataset also accounts for the missingness above by transforming certain variables like `rec_rank` (Recruiting rank from high school) from a numerical variable to a categorical one.


![Visualization of `pick` in the New Draft Data](images/draft_pic.png){#fig-draft}

Following the creation of an approxmiately uniform dataset for the variable `pick` that also accounted for missingness (shown by @fig-draft), predictive modelling was viable. Plus, each of the observations (unique players) correspond to over 60 different characteristics or statistics for appropriate and thorough analysis. More information about each of the recorded stats can be found in the `bball_players_codebook` text file within the `data` subdirectory.


## Methods


### Overview

Since a variety of predictor variables are utilized to predict whether a NCAA college basketball player is drafted or not, this prediction problem is identified as a simple classification problem.

### Data Splitting and Resampling

Before splitting the data, it was necessary to manipulate the dataset due to the large imbalance between college players that are drafted and those that are selected. First, created a `pick_dataset` that includes only the players that got drafted ("yes" value in the `pick` column). Then, I created a `non_pick_dataset` that includes only undrafted players, making sure to stratify the new dataset by `year` to fairly collect players from 2009-2021. With the `non_pick_dataset`, I then down sampled it to approximately match the number of observations in the `pick_dataset`. Finally, I utilized the `bind_rows()` function to combine both datasets into one complete dataset for analysis. 


Once this new suitable dataset for predictive modelling was created, the data was split so that 80% of the data could be utilized for training the models and 20% of the data could be utilized for testing the models. This split was conducted with stratified sampling by the variable `pick`, meaning there were two strata. 


Resampling was necessary to repeatedly estimate the population parameter for thorough analysis. This ensured random samples from the training data were chosen with replacement. Resampling was conducted by utilizing v-fold cross validation with 10 folds and 5 repeats. Also, this resampling process invovled stratifying the `pick` variable. In total, each model was fitted/trained 50 times. 


### Modelling and Tuning

In terms of model types, 6 different models were utilized below: 

1) Naive bayes model (`klaR`): This is my baseline model.

2) Logistic regression model (`glm`): This model is used because the dependent variable is binary, and it models the probability of an observation identifying with a certain class.


The following model types used some tuned hyperparameters with a regular grid to improve predictive capabilities:


3) Elastic net model (`glmnet`): This model uses both Ridge and Lasso to conduct a logistic regression. It selects certain features and accounts for noisy data to make predictions. 
    - penalty was tuned to prevent overfitting
    - mixture was tuned to act more like a Ridge or Lasso model for better accuracy


4) Boosted tree (`xgboost`): This model utilizes decision trees that account for results of previous trees to make new predictions.
    - min_n was tuned to control how many data points were split
    - mtry was tuned to control how many predictors were used
    - learn_rate was tuned to control the impact of new trees


5) K-nearest neighbors model (`kknn`): This model narrows the prediction features and uses Euclidean distance calculations. After that, it identifies the `K` closest points to predict values.
    - neighbors was tuned to control how many `K` points are utilized for predictions


6) Random forest model (`ranger`): This model utilizes decision trees based on random features. All the decision tree results are combined for predictions.
    - min_n was tuned to control how many data points were split
    - mtry was tuned to control how many predictors were used



### Recipes

A total of 5 recipes were created:


For the naive bayes model:

  - Basic Recipe (`nb_rec`): This preprocessing involved a standard kitchen sink recipe that utilized all the variables except those with zero variance, missingness issues, or were unrelated to the target variable. No dummy variables were used.


For the logistic regression and elastic net models:

  - Basic Recipe (`basic_rec`): This preprocessing involved a standard kitchen sink recipe that utilized all the variables except those with zero variance, missingness issues, or were unrelated to the target variable. Dummy variables were made out of the nominal variables and the data was appropriately scaled.
  
  - Variant Recipe (`off_rec`): This feature engineered recipe was created to evaluate whether a better predictive model could be produced by using solely utilizing offensive output for each basketball player. The thought process behind this recipe was that the NBA is often associated with pure offensive skill. Defense if often overlooked and it is generally thought that offensive talent and output in college translates best to the NBA. In order to test this, the preprocessing involved removing all variables unrelated to offensive output, converting nominal variables to dummy variables, imputing means for missing values in the numerical variables, correlating numerical variables that are related, creating an interaction between assist percentage and turnover percentage, appropriately scaling the data, and accounting for variables with zero variance.


For the boosted tree, k-nearest neighbor, and random forest models:

  - Basic Recipe (`trees_rec`): This preprocessing involved a standard kitchen sink recipe for trees that utilized all the variables except those with zero variance, missingness issues, or were unrelated to the target variable. Dummy variables were made out of the nominal variables and the data was appropriately scaled.

  - Variant Recipe (`off_trees`): This feature engineered trees recipe was created to evaluate whether a better predictive model could be produced by using solely utilizing offensive output for each basketball player. The thought process behind this recipe was that the NBA is often associated with pure offensive skill. Defense if often overlooked and it is generally thought that offensive talent and output in college translates best to the NBA. In order to test this, the preprocessing involved removing all variables unrelated to offensive output, converting nominal variables to dummy variables, imputing means for missing values in the numerical variables, correlating numerical variables that are related, appropriately scaling the data, and accounting for variables with zero variance.


### Evaluation Metric

Due to the classification problem at hand, the models will be compared with the accuracy metric. This metric measures the proportion of correct predictions out of total predictions. In order to effectively visualize the best model, a confusion matrix will be constructed.


## Model Building & Selection Results


### Overview

To reiterate, the accuracy metric will be utilized to compare all of the models to address the classification problem. A greater value of accuracy reflects a better model.

### Best Performing Model Results

```{r}
#| echo: false
#| label: tbl-accuracy
#| tbl-cap: Accuracy and Standard Error of Best Performing Models 

load("results/logreg_fit.rda")
load("results/nb_fit.rda")
load("results/rf_tune.rda")
load("results/bt_tune.rda")
load("results/knn_tune.rda")
load("results/elastic_tune.rda")

load("results/logreg_fit_off.rda")
load("results/rf_tune_off.rda")
load("results/bt_tune_off.rda")
load("results/knn_tune_off.rda")
load("results/elastic_tune_off.rda")


# make workflow set
model_results <- as_workflow_set(
  nbayes = nb_fit,
  logreg = logreg_fit,
  elastic = elastic_tune,
  rf = rf_tune,
  knn = knn_tune, 
  bt = bt_tune,
  logreg_off = logreg_fit_off,
  elastic_off = elastic_tune_off,
  rf_off = rf_tune_off,
  knn_off = knn_tune_off, 
  bt_off = bt_tune_off
)

# get highest accuracy for each model type
model_results |> 
  collect_metrics() |> 
  filter(.metric == "accuracy") |> 
  slice_max(mean, by = wflow_id) |> 
  distinct(wflow_id, .keep_all = TRUE) |>
  select(`Model Type` = wflow_id,
         `Accuracy` = mean,
         `Std Error` = std_err,
         `Num_Computations` = n) |>
  kable(digits = c(NA, 3, 4, 0))
```

@tbl-accuracy reveals the accuracy and standard error values for each model and the various recipes. It appears that the elastic net model with the basic (kitchen-sink) recipe is the best model because it corresponds with the greatest value for accuracy (91.4%) as well as the lowest standard error of all the models. 

#### Evaluation of Recipes

According to @tbl-accuracy, the newly feature engineered recipe, based on offensive output for each player, yielded a lower accuracy in terms of predictive modelling for all model types. This is because the accuracy metric reflects a lower value within each model type using the variant recipe compared to basic recipe. Also, three out of five of these models (with the variant recipe) have a lower value of accuracy when compared to the baseline `nbayes` model. Although it was initially thought that strictly offensive statistics would lead to better predictions, it appears that a range of factors (including defensive metrics) contribute towards better predictions for if a college basketball player is drafted. These results contradict that idea that the NBA is just an offensive showcase. Offensive output and talent may contribute towards receiving a selection in the NBA draft, but it is important to not overlook defensive abilities and the overall big picture of every NBA prospect. The entire profile of an NCAA college basketball player provides better overall predictive accuracy for getting drafted than strictly offensive output. Offensive talent in college does not directly translate to receiving a selection to the NBA as previously thought.


### Best Hyperparameters for the Models

```{r}
#| echo: false
#| label: tbl-hyper
#| tbl-cap: Best Set of Hyperparameters for the Models 

elastic_best <- elastic_tune |> 
  select_best(metric = "accuracy")

rf_best <- rf_tune |>
  select_best(metric = "accuracy")

knn_best <- knn_tune |>
  select_best(metric = "accuracy")

bt_best <- bt_tune |>
  select_best(metric = "accuracy")

elastic_off_best <- elastic_tune_off |> 
  select_best(metric = "accuracy")

rf_off_best <- rf_tune_off |>
  select_best(metric = "accuracy")

knn_off_best <- knn_tune_off |>
  select_best(metric = "accuracy")

bt_off_best <- bt_tune_off |>
  select_best(metric = "accuracy")


best_hyperparams <-
  bind_rows(
    elastic_best |>  mutate(model = "Elastic Net"),
    rf_best |>  mutate (model = "Random Forest"),
    knn_best |>  mutate (model = "K-nearest Neighbor"),
    bt_best |>  mutate (model = "Boosted Tree"),
    elastic_off_best |>  mutate(model = "Elastic Net (Off)"),
    rf_off_best |>  mutate (model = "Random Forest (Off)"),
    knn_off_best |>  mutate (model = "K-nearest Neighbor(Off)"),
    bt_off_best |>  mutate (model = "Boosted Tree (Off)")
  ) |>
  select(model,
         mtry,
         min_n,
         learn_rate,
         neighbors,
         penalty,
         mixture) |> 
  kable(digits = c(NA, 2, 2, 3, 2, 3, 2))

best_hyperparams
```

@tbl-hyper reveals the best hyperparameters for each of the models with the basic and (offensive) variant recipes. It is very interesting to see the differences in the best set of hyperparameters between models using the basic recipe compared to models using the feature engineered offensive output recipe. Values that retained similarity include the `learn_rate` for both boosted tree models, `mtry` for both boosted tree models as well as both random forest models, and `penalty/mixture` for both elastic models. For the future, tuning for the boosted tree and random forest models could be adjusted by increasing the value of `mtry`. Although the fitting time may increase, it appears as though the fitted models prefer to use as many predictors as possible to have the most accurate predictive capabilities.


### Final/Winning Model

The elastic net model, using the basic recipe for fitting, is the winning model because it had the greatest value for the accuracy metric. From @tbl-hyper, it can be concluded that the best hyperparameters were 0.001 for `penalty` and 0.75 for `mixture`. Essentially, this indicates that the most accurate elastic net model has a relatively weak regularization of Lasso or Ridge (penalty = 0.001) and a greater dependence on the Lasso term compared to the Ridge term (mixture = 0.75). This resulting prediction model is somewhat surprising because this model seems to be a more simple model than some of the complex ones and relies heavily on the logistic regression. However, many of the predictor stats, at least the numeric variables, may have had strong correlation with being drafted into the NBA, so it's not extremely surprising. Again, it is surprising that the feature engineered recipe was less accurate for all the models than the basic recipe, but it reveals that offensive output for a college player may not be as important as the overall picture in terms of receiving a selection by a NBA team in the draft.


## Final Model Analysis


### Results of Fitting Winning Model to Testing Data

```{r}
#| echo: false
#| label: tbl-final
#| tbl-cap: Accuracy for Final Elastic Net Model on Testing Data 

load("results/final_fit.rda")

bball_test <- bball_test |> 
  mutate(pick = factor(pick))

# make metric set
bball_metric <- metric_set(accuracy)

# tibble of predicted values
en_predict <- bball_test |> 
  bind_cols(predict(final_fit, bball_test)) |> 
  select(pick, .pred_class)

# apply metrics to predictions
bball_metric(en_predict, truth = pick, estimate = .pred_class) |> 
  select(.metric, .estimate) |> 
  kable(align = "c", col.names = c("Metric", "Estimate"), 
        caption = "Evaluation of Predictions by Elastic Net using Accuracy")
```

@tbl-final refects the accuracy metric for the winning model. It reveals that the final elastic net model was approximately 92.31% correct when predicting whether a NCAA college basketball player was drafted into the NBA or not within the testing data. Generally, this means that the best elastic net model was 92.31% correct when predicting whether a collegiate player is drafted or goes undrafted into the NBA.


### Visualization of Final Model's Accuracy

![Heatmap of Predictions vs. True Values for Final Model](images/conf_mat_heatmap.png){#fig-heat}

@fig-heat shows that the false positives (prediction that a player is drafted but the real outcome is undrafted) can be found on the bottom left (35) while the false positives (prediction that player goes undrafted but the real outcome is drafted) can be found on the top right (10). For appropriate analysis, "yes" means a player was drafted, while "no" means a player went undrafted. Within the matrix above, there are 277 "yes-yes" values which means that the model was correct at predicting a player receiving a selection into the NBA 277 times. Meanwhile, there was 263 "no-no" values, which reflects the opposite. This means the model was correct at predicting a player going undrafted into the NBA 263 times.

Due to the high accuracy value for the final model, presented in @tbl-final, this winning elastic net model is good at addressing the prediction problem. The effort put in to construct this predictive model was worth it because it proved much better than the baseline naive bayes model. The difference in accuracy metric was 92.3% compared to 84.8% for the baseline model. This indicates a 7.5% difference in correctly predicting whether a NCAA college basketball player is drafted into the NBA or not.


#### Discussion of Target Variable

Even though the target variable `pick` was transformed from a numerical variable to a categorical variable, it is not necessary to conduct an analysis of the original numerical scale of this variable. This is because the prediction problem was presented as a classification problem, so the constructed models and utilized accuracy metrics would not be predictive of the original `pick` variable on a numerical scale.


## Conclusion


### Final Interpretation

The best model in terms of predictive capacity for evaluating whether a NCAA college basketball player is selected in the NBA draft is the elastic net model with the basic (kitchen sink) recipe. This elastic net model has a relatively weak regularization of Lasso or Ridge (penalty = 0.001) and a greater dependence on the Lasso term compared to the Ridge term (mixture = 0.75). The performance of this model was justified by the greatest accuracy value of all the models.

### Future Directions 

In order to further improve the model's performance, a greater subset of players could be analyzed. The current dataset was limited to players between 2009 to 2021, so increasing the number of players for evaluation for past years could yield better predictive power.

One limitation for further analysis is how lobsided the number of college basketball players that go undrafted compared to those that are drafted. This issue makes it difficult to naturally collect uniform data for predictive modelling. Collection of more observations (players) could help address this problem.

Also, the NBA Draft has recently shifted over to drafting many more players from overseas. This drastically limits the already small number of NCAA collegiate players that get drafted. A possible next step could be also including foreign basketball prospects to aid in the predictive power for the future.

Finally, a new proposed research project could entail evaluating a college player/team's success in March Madness with getting drafted. Since this is a big stage, full of high pressure situations, performance during March in college basketball could possibly be used to predict whether a college player is drafted. This also helps achieve more uniform data because players participating in the NCAA tournament are within the top 68 teams in the country.



## References

For my analysis above, I am utilizing a singular dataset called `CollegeBasketballPlayers2009-2021.csv` to compare over 4500 college basketball players from 2009 to 2021.

The source of my data is a user on `Kaggle.com`, named Aditya Kumar. They are a computer science major at the University of Southern California with roots in Los Angeles, California. The data card is named `College Basketball 2009-2021 + NBA Advanced Stats`^[Found on [https://www.kaggle.com/datasets/adityak2003/college-basketball-players-20092021/data](https://www.kaggle.com/datasets/adityak2003/college-basketball-players-20092021/data)]. Although all the stats were compiled by Aditya, the raw statistical data should be credited to Bart Torvik^[Found on [https://barttorvik.com/](https://barttorvik.com/)].


### Citations

Kumar, A. (2022). College Basketball 2009-2021 + NBA Advanced Stats. Kaggle.com. [https://www.kaggle.com/datasets/adityak2003/college-basketball-players-20092021/data](https://www.kaggle.com/datasets/adityak2003/college-basketball-players-20092021/data)


Torvik, B. (2024). Year Customizable T-Rank and Tempo-Free Stats. https://barttorvik.com/. [https://barttorvik.com/playerstat.php?link=y&year=2021&start=20201101&end=20210501](https://barttorvik.com/playerstat.php?link=y&year=2021&start=20201101&end=20210501)

